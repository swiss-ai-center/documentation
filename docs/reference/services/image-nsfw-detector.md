# image-nsfw-detector

- [:material-account-group: Main author - HEIA-FR](https://www.hes-so.ch/swiss-ai-center/equipe)
- [:material-git: Code](https://github.com/swiss-ai-center/image-nsfw-detector-service)
- [:material-kubernetes: Deployment configuration](https://github.com/swiss-ai-center/image-nsfw-detector-service/tree/main/kubernetes)
- [:material-test-tube: Staging](https://image-nsfw-detector-swiss-ai-center.kube.isc.heia-fr.ch)
- [:material-factory: Production (not available yet)](https://image-nsfw-detector.swiss-ai-center.ch)

This service takes as input an image and returns a json with information about the probability that it includes NSFW content.

## NSFW content detection

NSFW stands for *not safe for work*. This Internet slang is a general term associated to un-appropriate content such as nudity, pornography etc. See e.g. https://en.wikipedia.org/wiki/Not_safe_for_work. It is important to exercise caution when viewing or sharing NSFW images, as they may violate workplace policies or community guidelines.

The current service encapsulates a trained AI model to detect NSFW images with a focus on sexual content. Caution: the current version of the service is not able to detect profanity and violence for now.

### Definition of categories

The border between categories is sometimes thin, e.g. what can be 
considered as acceptable nudity in some cultural context would be considered as 
pornography by others. Therefore we need to disclaim any complaints that would
be done by using the model trained in this project. We can't be taken responsible
of any offense or classifications that would be falsely considered as appropriate 
or not. To make the task even more interesting, we went here for two main 
categories *nsfw* and *safe* in which we have sub-categories.

- **nsfw**:
  - **porn**: male erection, open legs, touching breast or genital parts, 
  intercourse, blowjob, etc; men or women nude and with open legs fall into
  this category; nudity with sperma on body parts is considered porn
  - **nudity**: penis visible, female breast visible, vagina visible in 
  normal position (i.e. standing or sitting but not open leg)
  - **suggestive**: images including people or objects making someone think 
  of sex and sexual relationships; genital parts are not visible otherwise
  the image should be in the porn or nudity category; dressed people kissing 
  and or touching fall into this category; people undressing; licking 
  fingers; woman with tong with sexy bra
  - **cartoon_sex**: cartoon images that are showing or strongly 
  suggesting sexual situation
- **safe**:
  - **neutral**: all kind of images with or without people not falling 
  into porn, nudity or suggestive category
  - **cartoon_neutral**: cartoon images that are not showing or  
  suggesting sexual situation

Inspecting the output giving probabilities for the categories (safe vs not-safe) and
the sub-categories, the user can decide where to place the threshold on what is 
acceptable or not for a given service.

### Data set used to build the model

A dataset was assembled using existing NSFW image sets and was completed with web scraping data.
The dataset is available for research purpose - contact us if you want to have an access. Here
are some statistics about its conent (numbers indicate amount of images). The dataset is balanced among
the categories, which should avoid biased classifications.

| categories     | safe    |        |         | nsfw       |        |      |         | total   |       |       |
|----------------|---------|--------|---------|------------|--------|------|---------|---------|-------|-------|
| sub-categories | general | person | cartoon | suggestive | nudity | porn | cartoon | safe    | nsfw  | all   |
| v2.2           | 5500    | 5500   | 5500    | 5500       | 5500   | 5500 | 5500    | 16500   | 22000 | 38500 |

### Model training and performance

We used transfer learning on MobileNetV2 which present a good trade-off between performance and runtime efficiency.

| Set  | Model                                                   | Whole |       | Val   |       | Test  |       |
|------|---------------------------------------------------------|-------|-------|-------|-------|-------|-------|
|      |                                                         | sa/ns | sub   | sa/ns | sub   | sa/ns | sub   |
| V2.1 | TL_MNV2_finetune_224_B32_AD1E10-5_NSFW-V2.1_DA2.hdf5    |       |       | 95.7% | 85.1% | 95.7% | 86.1% |

In this Table, the performance is reported as accuracy on the safe vs not-safe (sa/ns) main categories and
on the sub-categories (sub). The sub performance in indeed lower as we have naturally more confusion between
some categories and as there is simply a larger cardinality in the number of classes.

The API documentation is automatically generated by FastAPI using the OpenAPI
standard. A user friendly interface provided by Swagger is available under the
`/docs` route, where the endpoints of the service are described.

This service only has one route `/compute` that takes an image as input, which will be used to detect NSFW content.

## Environment variables

All environment variables are described in the
[`.env`](https://github.com/swiss-ai-center/image-nsfw-detector/blob/main/.env) file.

The environment variables can be overwritten during the CI/CD pipeline described
in the
[`image-nsfw-detector.yml`](https://github.com/swiss-ai-center/image-nsfw-detector/blob/main/.github/workflows/image-nsfw-detector.yml)
GitHub workflow file.

## Start the service locally with Python

In the `image-nsfw-detector` directory, start the service with the following commands.

```sh
# Generate the virtual environment
python3 -m venv .venv

# Activate the virtual environment
source .venv/bin/activate

# Install the requirements
pip install \
    --requirement requirements.txt \
    --requirement requirements-all.txt
```

Start the application.

```sh
# Switch to the `src` directory
cd src

# Start the application
uvicorn --reload --port 9090 main:app
```

Access the service documentation on <http://localhost:9090/docs>.

## Run the tests with Python

For each module a test file is available to check the correct behavior of the
code. The tests are run using the `pytest` library with code coverage check. To
run the tests, use the following command inside the `image-nsfw-detector` folder:

```sh
pytest --cov-report term:skip-covered --cov-report term-missing --cov=. -s --cov-config=.coveragerc
```

## Start the service locally with minikube and the Docker image hosted on GitHub

Start the service with the following commands. This will start the service with
the official Docker images that are hosted on GitHub.

In the `image-nsfw-detector` directory, start the service with the following commands.

```sh
# Start the image-nsfw-detector backend
kubectl apply \
    -f kubernetes/config-map.yml \
    -f kubernetes/stateful.yml \
    -f kubernetes/service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The
terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `image-nsfw-detector` documentation on <http://localhost:9090/docs>.

Access the Core engine documentation on <http://localhost:8080/docs> to validate
the backend has been successfully registered to the Core engine.

## Start the service locally with minikube and a local Docker image

**Note**: The service StatefulSet (`stateful.yml` file) must be deleted
and recreated every time a new Docker image is created.

Start the service with the following commands. This will start the service with
the a local Docker image for the service.

In the `image-nsfw-detector` directory, build the Docker image with the following commands.

```sh
# Access the Minikube's Docker environment
eval $(minikube docker-env)

# Build the Docker image
docker build -t ghcr.io/swiss-ai-center/image-nsfw-detector-service:latest .

# Exit the Minikube's Docker environment
eval $(minikube docker-env -u)

# Edit the `kubernetes/stateful.yml` file to use the local image by uncommented the line `imagePullPolicy`
#
# From
#
#        # imagePullPolicy: Never
#
# To
#
#        imagePullPolicy: Never
```

In the `image-nsfw-detector` directory, start the service with the following commands.

```sh
# Start the image-nsfw-detector backend
kubectl apply \
    -f kubernetes/config-map.yml \
    -f kubernetes/stateful.yml \
    -f kubernetes/service.yml
```

Create a tunnel to access the Kubernetes cluster from the local machine. The
terminal in which the tunnel is created must stay open.

```sh
# Open a tunnel to the Kubernetes cluster
minikube tunnel --bind-address 127.0.0.1
```

Access the `image-nsfw-detector` documentation on <http://localhost:9090/docs>.

Access the Core engine documentation on <http://localhost:8080/docs> to validate
the backend has been successfully registered to the Core engine.